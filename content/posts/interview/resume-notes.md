---
title: "Resume Notes"
date: 2024-03-02T21:06:07+08:00
topics: "algorithms-and-data-structures"
draft: false
tags: ["leetcode"]
summary: "简历注释"
---

# 简历注释

## 项目&竞赛分析

### OB-Bootstrap启动优化

#### 介绍

背景：OB决赛，单机场景下的bootstrap优化。空集群初始化为bootstrap.

1. 因为主要通过日志插桩为主要的调试和分析手段。首先用shell/python编写日志转换、过滤、可视化脚本。比如通过时间段、模块名、TraceId、模式匹配等方式过滤日志。

2. 通过源码阅读（日志、调试辅助）分析具体执行过程，耗时原因。然后来设计方案。

    在源码阅读过程中绘制了大量的函数调用关系和流程图。

    比如单机下的快速选举流程，分析OB的Paxos选举算法，定位耗时原因（主要是时间窗口等待，以及忙等），抽取选举成功的leader上任代码，在membership为1时直接上任。（没有考虑membership改变的问题）

    Plan cache的分析、各种忙等循环位置的分析、Multi-schema的分析

3. 完备的测试过程

    引入的各种修改太多了，为避免修改代码引入bug，编写批量测试脚本，修改后的代码至少需要跑三次测试脚本，并记录好各个部分的减少时间，便于查找问题。
    * 比如测试失败了，都不知道是什么时候引入的代码导致的，bug累积没法再继续优化下去
    * 比如某次修改后，突然重启时间超时了，但这可能是多次修改导致的结果，没法定位导致超时的原因
    
*波折*

正如上面所说，最开始我们是没有把测试当回事的，都是手动测试一下，有时间减少了就把修改的内容记录一下。

但当我们优化到10s的时候，没法再继续优化下去了。很多bug，每次提交出现的bug可能都不同，特别是本地还无法复现。

尽管本地还可以继续做优化，但线上测试已无法通过。

所以才有这里的，完备的测试过程。

重新拉去分支，重新应用每个修改，进行完备的测试，及时发现问题，同步进行线上提交环境测试。

使用限制CPU数量、频率的方式模拟线上测试环境，尽可能复现bug。

> 同步在比赛官方提供的虚拟机上也进行测试（但这个太慢了，编译一次动不动就是半小时一小时）

花了一周的时间，重新修改和复现，再次应用完所有修改后已可达到6s，并具有充分的稳定性。

> 我们只有2/3百次提交，其他靠运气提交有上千次提交。

然后就是通宵+两天，尝试使用破话性手段继续优化(异步化)。

完备的测试，为这些破坏性手段提供了冗余，比如正是由于之前的修改尽量保证不延长（占用）sysbench和restart的时间，才为破坏性修改提供了可能，不然线上提交无法通过，时间也无法减少。

#### 问题

整个OB决赛更多的是阅读和理解OB的代码，真正改的其实不多。

*还可以优化的地方？*

破坏性太强，在比赛这种竞赛环境下可以，实际使用需要设计更加完备和优雅的方案。

OB毕竟是一个成熟完备的数据库，还需要更深入的了解其中设计思想和原理，才能更好的修改。

*各部分优化的时间？*

单节点选举减少了约24s，调整锁忙等、各种忙等时间大概减少了接近40s，并行化建表、并行化CreateTenant大概10s，破坏性手段4-5s

### MiniOB功能实现

#### 介绍

背景：OB初赛

主要是在一个小型的数据库上实现经典的数据库功能。MiniOB把一个单机数据库的框架给搭起来了，比如底层的文件页面管理、Bufferpool、简单的MVCC事务模型、简单的火山模型的物理计划、逻辑计划、简单查询优化、表达式执行、词法语法解析等。

但有很多功能都未实现，比如子查询、复杂子查询、聚合函数和Group By以及Having、Order By、表达式功能函数（日期函数等）等等。

任务就是在该数据库上实现这些功能。

* 使用面向对象思想重构SQL AST，并重构词法语法解析，使其SQL解析能力更全面，且更具扩展性。
* 设计和实现统一SQL表达式生成和执行方案，使表达式支持（嵌套）子查询、函数、聚合函数以及类型检查。

单独设计的表达式执行树，表达式中内嵌子查询的带物化算子的物理查询计划。

> 非depend的子查询只需要执行一次即可。

多重tuple组成的tuple上下文（context），自动组成多重名字空间，便于复杂子查询的执行。

比如，Primary中的where存在一个复杂子查询，其中对于这个子查询的where的表达式来说，其在执行时，它可以通过TupleContext（一个链表从末尾到头部，分别是当前子查询的查出来的tuple->父查询传递进来的tuple），只需要在计算时查找该TupleContext即可获取到父查询的当前Tuple值。

* 通过递归实现复杂多重子查询（包含别名）语义分析，并生成 SelectStmt
* 重构 Group By 、聚合函数、复杂表达式、 Order By 、投影等功能的逻辑计划和物理计划。
* 实现B+树多字段索引与唯一索引支持

*波折*

没有按照赛题一个一个实现，而是按照Select语句自身的功能，一点点重构以支持复杂功能，避免多次重复修改。

但一步到位（完全支持复杂表达式，复杂子查询并提供扩展性）复杂度稍高，曲线较陡峭。

并且需要完全重构，从最底层的Tuple结构到表达式计算、物理计划、逻辑计划（需要支持复杂子查询执行）、语义分析、词法语法。

#### 问题

### 分布式一致性KV

#### 介绍

1. Basic Raft的实现

> 容错的日志一致性复制协议，日志一致，确定性状态机的执行结果则一致。

* 强主复制，需要一个Leader来作为请求的入口，负责将日志复制到Follower

* Leader选举算法，通过多数派投票确定当前最新Term的leader，同时投票者需要对比请求成为Leader的日志和本地日志号进行对比，确保选举出的leader具有最新以提交的日志。对于投票结果还需要进行持久化，避免重启后重复投票，造成同一Term有多个Leader，且任期混乱。

> 忽略旧Term保证旧Leader失效，随机timeout避免分票

* 向Leader提交Log，Leader向Follower进行批量AppendLog（强制复制），多数派Follower收到Log后更新CommitIndex。异步循环检查appliedIndex和CommitIndex，通过channel向上层提交。

> AppendLog过程中，只有preLog的Term和Index都对的上，才Append成功，不然失败，使用快速backup更新Leader的matchIndex

> 复制但不提交旧Term日志，Figur8问题，旧Term日志如果被提交，同LogIndex可能已存在更大Term的Log，之后再重新选举Leader，将导致被提交的旧Term日志被覆盖，导致已提交log被破坏。

* 通过Snapshot压缩log，由上层应用确定压缩时机（一般是log长度或者大小超过阈值），freeze当前状态机状态，记住对应的appliedIndex，通过类似写时拷贝(双map，快照生成完成后再合并结果)，保证状态机快照生成，与新apply log不冲突。然后将生成快照传递到Raft模块，由Raft删除日志。新增InstallSnapshotRPC，在需要已裁剪的log时，向其发送快照，应用到上层状态机。

2. 使用Raft实现一致性KV存储

一个假设：Client发完一个，服务返回成功后，才发送下一个。失败，则重新找Leader，发送请求，直到成功。

故在该简单假设下，只需为每个Client以及每个请求分配一个Id，

然后在服务端保存当前执行完成的请求的Id，以及执行结果，并将该信息同样利用Raft同步即可实现线性一致性。

向Leader发送请求，Leader将改请求挂起（挂在一个chanel上），然后等待Log被Applied，找到对应挂起的请求，使用Channel通知它。

#### 问题

* 哪些困难？

corner case调试困难，只能通过日志去分析。

比如幂等性的实现，异步请求，收到响应需判断当前状态。部分重叠的AppendRPC导致的多数派统计问题。

关于Figure8的解决方案，这里使用的只复制但不提交旧TermLog，但这会导致一个问题，如果一致没有新Log到来，最后一条旧Log永远也不会被提交。比如所有宕机了，再重新选出的Leader，将永远无法Commit该log，如果没有新log到来的话。

最佳的解决方案应该时，新leader在上任时插入一条新term的No-op日志。但该Lab测试程序要求Log Index必须连续，故该方案不可行。

故在实现该一致性KV时，采用超时的方案，如果某个Client的请求在leader挂起的时间太久（通过select定时器实现），向Client返回失败，Client会自行重新提交，这将会是新Termlog，但此时Leader的log条目中存在重复Log，通过Server记住的当前已执行的最新MsgID过滤掉重复ID。

* 一致性怎么实现的？

一个假设：Client发完一个，服务返回成功后，才发送下一个。失败，则重新找Leader，发送请求，直到成功。

故在该简单假设下，只需为每个Client以及每个请求分配一个Id，

然后在服务端保存当前执行完成的请求的Id，以及执行结果，并将该信息同样利用Raft同步即可实现线性一致性。

* 读优化？

Read Index(避免同步日志那些操作，但是任然要经过一次心跳的RTT)

Lease Read(心跳更新确认自己是leader，则维护租约一直到now+elect_timeout, 但每台机器的时钟速度必须一致)

类似Zookeeper的减弱一致性，去实现从副本读。

### 分布式图存储引擎

#### 介绍

背景：涉密项目，可以理解为特定场景设计的一个简单的大规模分布式图存储和查询引擎。

图数据量大，分区在多节点存储，提供更大容量，同时在每个分区上并行查询提高性能。

分区方式：利用子图聚集算法，切分图，保证每个分区的图数据尽可能具有邻近关系。

每个分区中的图节点和边ID中有一部分位用来表示是哪个分区。

另有元数据服务器（可用Zookeeper/etcd保证容错能力），管理每个分区在哪个服务器上的映射关系。

大致的分区存储完成后，则可进行分布式图查询，主要是分布式快速图遍历算法。

采用部分存算分离的结构，上层的查询控制器会下推部分算子到分布式图存储引擎执行，加快执行速率减少数据传输。

比如，一个简单的图节点查询，上层下推该算子到分布式图存储引擎，任意一个Server成为当前Task的Master，然后广播该请求，在每个分区上执行查询和过滤，再回传给查询控制器。

再如，一个多圈层查询算子，同样一个Server成为Task Master，先广播查询初始点，查出来后，每个分区再根据其本分区的图拓扑关系查找边（关系），然后可通过该边的对端节点ID判断，下一跳节点是否在本分区，不在则需要根据元数据信息，路由该查询任务该对应分区的服务器。最后均完成查询后，在查询控制器组装结果。

其他还有诸如multi-raft保证多副本（元数据服务维持哪个分区在哪个raft group），事务控制（2PC+MVCC）主要面向OLAP事务这块做的工作不多，粒度比较粗。

#### 问题

### 基于计算机视觉的交通场景应用

#### 介绍

#### 问题

## 专业技能若干

待续
